{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection\n",
    "\n",
    "When you open camera app on your mobile phone to take someone's photo, it automatically detects all the faces in the image and makes a yellow box around all the detected faces. Not just camera app, face detection is everywhere. Facebook automatically detects all the faces in the images and suggests your names while tagging. \n",
    "\n",
    "As an exercise, try finding some more examples of face detection. You will be surprised."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Haar Cascade Classifiers\n",
    "\n",
    "In order to build face recognition application, we will use the built-in Haar cascade classifiers in OpenCV. These classifiers have already been pre-trained to recognize faces!\n",
    "\n",
    "Building our own classifier is certainly outside the scope of this case study. But if we wanted to, we would need a lot of “positive” and “negative” images. Positive images would contain images with faces, whereas negative images would contain images without faces. Based on this dataset, we could then extract features to characterize the face (or lack of face) in an image and build our own classifier. It would be a lot of work, and very time consuming. Things are even more difficult for someone, who is a computer vision novice. Luckily, OpenCV will do all the heavy lifting for us.\n",
    "\n",
    "**Working**\n",
    "\n",
    "Haar-Cascade classifiers work by scanning an image from left to right, and top to bottom, at varying scale sizes. Scanning an image from left to right and top to bottom is called the “sliding window” approach. As the window moves from left to right and top to bottom, one pixel at a time, the classifier is asked whether or not it “thinks” there is a face in the current window, based on the parameters that are supplied to the classifier.\n",
    "\n",
    "Lets write some code . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('../images/friends.jpeg')\n",
    "imshow('Image', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am a big fan of FRIENDS and hence big image, let's resize it so that its much eaiser to visualize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w, _ = img.shape\n",
    "img = cv2.resize(img, (int(w*0.8), int(h*0.8)))\n",
    "imshow('Image', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "much better now. We took the initial dimensions and then multiplied them with 0.8. The `cv2.resize` function take an image and the new dimensions i.e 80% of the previous width & height.\n",
    "\n",
    "**Note:** We switched the order of *width* and *height*. Because when we say `img.shape` it returns the dimensions as per numpy cordinate system, but when we are using `cv2.resize` function, its expects the dimensions in opencv cordinate system.\n",
    "\n",
    "Lets now save the resized image for future use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('images/friends2.jpeg', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To detect faces in an image, we will have to convert the image into grayscale first. Infact, converting a color image to gray-scale is one of the most frequently used operation in computer vision. OpenCV has a handly little function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "imshow('Gray', gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument is always the image and second argument tells opencv about the current color space and the new color space to which the image is to be transformed.\n",
    "\n",
    "You can use the same function to transform the image to other color spaces as well. By default, OpenCV uses BGR color space. To convert an image to RGB color space . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "imshow('RGB', img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to HSV color space . . . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "imshow('HSV', img_hsv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read more about color spaces and their importances, [here](https://www.dynamsoft.com/blog/insights/image-processing-101-color-models/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "continuing with the problem in hand . . . \n",
    "We have a black and white image now. Lets now talk about classifier.\n",
    "\n",
    "Haar-Cascade classifiers are serialized as an XML file. You can easily load them using `cv2.CascadeClassifier()` method. This method take the path to the *xml* file as input and returns a classifier object.\n",
    "\n",
    "**Note:** You can find *xml* files for a lot of other classifier in the official git repo of opencv, [here](https://github.com/opencv/opencv/tree/master/data/haarcascades) \n",
    "\n",
    "We have already download the *xml* file for detecting faces from the git repo. Its present inside data directory, lets try loading it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('data/face.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it ran without any error.\n",
    "\n",
    "To detect actual faces in the image we make a call to the `detectMultiScale` method of our classifier. The method takes care of the entire face detection process. The method takes one required parameter, the image that he wants to find the faces in, followed by three optional arguments:\n",
    "\n",
    "- `scaleFactor`:  How much the image size is reduced. A value of 1.05 indicates that the image will by reduced by 5%\n",
    "-  `minNeighbors`:  How many neighbors each window should have for the area in the window to be considered a face. The cascade classifier will detect multiple windows around a face. This parameter controls how many rectangles (neighbors) need to be detected for the window to be labeled a face.\n",
    "- `minSize`:  A tuple of width and height (in pixels) indicating the minimum size of the window. Bounding boxes smaller than this size are ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.15, minNeighbors=9, minSize=(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `detectMultiScale` method then returns *a list of tuples containing the bounding boxes of the faces* in the image. These bounding boxes are simply the *(x, y)* location of the face, along with the *width* and *height* of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[272, 260,  91,  91],\n",
       "       [590,  77,  98,  98],\n",
       "       [734, 245, 102, 102],\n",
       "       [638, 274, 106, 106],\n",
       "       [503,  24,  94,  94],\n",
       "       [389, 148,  96,  96]], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our classifier has detected 6 faces in the image. We can draw one of them using `cv2.rectangle` function that we learned in the last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, w, h = faces[0]\n",
    "upper_left_corner = (x, y)\n",
    "lower_right_corner = (x+w, y+h)\n",
    "color = (0, 255, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets draw our rectangle on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.rectangle(img, upper_left_corner, lower_right_corner, color, 2)\n",
    "imshow('Image', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we have detected one face, now lets try to draw rectangles for all the faces that are detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 255, 0)\n",
    "\n",
    "for face in faces:\n",
    "    x, y, w, h = face\n",
    "    upper_left_corner = (x, y)\n",
    "    lower_right_corner = (x+w, y+h)\n",
    "    img = cv2.rectangle(img, upper_left_corner, lower_right_corner, color, 2)\n",
    "    \n",
    "imshow('Image', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazing, we detected all the faces in the image. It looks a lot of code in the first go but actually its much simplier. I have written the complete code again, in a single cell. See if you can understand it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('images/friends2.jpeg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('data/face.xml')\n",
    "faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.15, minNeighbors=9, minSize=(28,28))\n",
    "\n",
    "color = (0, 255, 0)\n",
    "\n",
    "for face in faces:\n",
    "    x, y, w, h = face\n",
    "    img = cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "    \n",
    "imshow('Image', img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its just ~10 lines of code. And the core logic is just ~3 lines. Sometimes, its really mind boggling how much you can achieve with just few lines of code.\n",
    "\n",
    "It would be great to take this a step further and make it real time. We will take images from our webcam and detect faces, all in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time face detection\n",
    "\n",
    "First we need to access our webcam. With opencv, its just a function call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argument **0** means read from built-in or USB webcam. You can read a video file from the disk by simply passing its path, instead of **0**. \n",
    "\n",
    "Assuming that grabbing a reference to the video was successful, we can easily read the current frame by calling `read()` method of our *camera* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = camera.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`camera.read()` returns two values. The first value is a boolean, indicating whether the frame capture was successful or not. The second value is the actual frame/image captured. Lets display the image we have captured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow('Frame', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to capture a single frame from the camera, we can easily loop over all frames in the video. At the most basic level, a video is simply a sequence of images put together, implying that we can read these frames one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    ret, img = camera.read()\n",
    "    cv2.imshow('video',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == ord('q'): # press 'q' to quit\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever you write an infinite while loop, you have to implement the *break* condition. Here, the *break* condition looks a bit different. Lets break it down.\n",
    "\n",
    "`cv2.waitKey` waits for a key press. Its takes one optional argument, *delay* in milliseconds. 0 is the special value that means “forever”.\n",
    "The function waitKey waits for a key event infinitely or for delay milliseconds, when it is positive. Here, we are using `cv2.waitKey(30)` it will display the frame for 30 ms, after which display will be automatically closed.\n",
    "\n",
    "If in that 30 ms, you press any key then `cv2.waitKey` will return the interger corresponding to the key that was pressed. You can simply *&* the returned value with *0xff*. *0xff* is a hexadecimal constant which is 11111111 in binary. By using bitwise AND (&) with this constant, it leaves only the last 8 bits of the original (in this case, whatever cv2.waitKey(0) is). \n",
    "\n",
    "Once you have the key value, you can check if it was *q* or not. If the key pressed was *q* then you can break the loop. The `ord()` function in Python accepts a character as an argument and returns the unicode code point representation of the passed argument. For example, in the above code, ord('q') returns 113 which is a unicode code point value of character 'q'.\n",
    "\n",
    "It might look like a lot to remember and understand but, trust me, its very easy and you will get use to it pretty soon. \n",
    "\n",
    "Final thing! In programming, whenever you access an I/O device like camera, USB, database and even files; you will have to close it explicitely when you are done using it. Here, we are capturing frames from our webcam. So, we will have to close it once we are done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a general practive to release the camera and then destroy all the windows. So, you will always see these commands used together. \n",
    "\n",
    "Lets put everything back together. See if you understand it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = camera.read()\n",
    "    if ret:\n",
    "        cv2.imshow('video',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == ord('q'): # press 'q' to quit\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the code you need to read images from your webcam and display them. You can also save the images by calling `cv2.imwrite`. \n",
    "\n",
    "We already know how to find faces in a single image. Since, inside the loop we have access to individual frames we easily use the same code from above the detect faces in each frame and then display them. Lets see it in action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 255, 0)\n",
    "face_cascade = cv2.CascadeClassifier('data/face.xml')\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = camera.read()\n",
    "    if ret:\n",
    "        ## Code to detect faces\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)        \n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.15, minNeighbors=9, minSize=(28,28))\n",
    "\n",
    "        for face in faces:\n",
    "            x, y, w, h = face\n",
    "            img = cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "        cv2.imshow('video',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == ord('q'): # press 'q' to quit\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't be explaining the code again, because there is nothing new. Spend a minute or two read the complete code. If you have any doubt you can ask your mentor.\n",
    "\n",
    "**Note:** One thing to notice here is, how easy it is to make a real-time application if you have the code to process an image. This is the case with all the computer vision applications that we build, we start with a single image and write all the code for it. Finally, to make it a real-time application, we simple grab the camera object and wrap an infinte while loop around it.\n",
    "\n",
    "This is all the extra code you need to make something real-time in opencv\n",
    "\n",
    "```python\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, img = camera.read()\n",
    "    if ret:\n",
    "        process_img(img) # only this function is updated, depending upon the application\n",
    "        cv2.imshow('video',img)\n",
    "\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k == ord('q'): # press 'q' to quit\n",
    "        break\n",
    "        \n",
    "camera.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "```\n",
    "\n",
    "Its amazing, right?! Don't worry if you did not understand everything now, we will be building many applications throughout the course. You will easily get accustom to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questionaire\n",
    "- How to resize images?\n",
    "- Name a few color spaces and also how to convert images from one color space to another?\n",
    "- What are haar cascade classifier and how they work?\n",
    "- How to read video input: webcam or mp4 from disk?\n",
    "- Difference between `cv2.waitkey(0)`, `cv2.waitkey(30)` and `cv2.waitkey(100)`\n",
    "- Why is closing I/O devices important?\n",
    "- Which command is used to release camera?\n",
    "\n",
    "If you don't know the answer to any of these question, then please read the notebook again or ask your mentor. There are some questions that your are suppose to google and read to know more.\n",
    "\n",
    "In the next notebook, we will learn about image transformations like rotation, scaling, translation, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
